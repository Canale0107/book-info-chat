# 構想

## 1. 私は何を作りたいか（ゴールの定義）

作りたいもの（プロダクト像）

「自然言語で会話すると、本を推薦・検索してくれるチャットボット」

- 雑談（気分/興味/悩み/テーマ）から
- botがユーザの意図を推定して
- 本の検索を実行し（CiNii API）
- 読みやすく整理して おすすめを返す
- さらに「もう少し軽め」「もっと哲学寄り」などの会話で 絞り込み・再推薦していく

今回の主眼（技術的テーマ）

私が今回やりたいのは特にここ：

1. AIモデルのAPIを使う
2. Function calling（Tool calling）で外部APIを呼ぶ
3. 独自チャットボットとして公開する

つまり「LLMアプリ（AIエージェント的）を作る練習」＋「ちゃんと公開できる形」を作りたい。

## 2. 何を使って本を検索するか（CiNii / Amazon）

結論：基盤は CiNii が良い

- 書誌情報の検索に強い
- ownerCount（所蔵館数）など推薦理由にできる情報がある
- OpenSearchの仕様が明確で、APIツールとして扱いやすい

Amazonはどう扱うのが良いか

- Amazonは「買う情報」が強いが、公式API運用は制約がある
- なので最初は **Amazonは補助（リンク）**でOK

✅おすすめの方針：
CiNiiを“正”の書誌データとして扱い、Amazonは「購入/検索リンク」程度に付ける
（両方を紐付けるならISBNをキーにする）

## 3. どう作るのが良いか（構成・技術スタック）

私が目指す B〜C（運用できる公開→拡張可能）に最適な形はこれ。

推奨アーキテクチャ（B→C向け）

✅ フロント：Next.js（TypeScript）

- 公開用のUIが作りやすい（チャット、検索結果カード、絞り込み、履歴）
- “プロダクト感”が出る
- 後から機能追加しやすい

✅ バックエンド：FastAPI（Python）

- 私が使ってみたい
- CiNii APIを呼ぶ「検索プロキシ」役に最適
- appid秘匿・タイムアウト・キャッシュなど運用要素を入れやすい

✅ LLM：まずは OpenAI API（公式）

- Tool callingが素直に動かしやすい
- 「LLM × 外部API呼び出し」の学習が最短距離

（後から Bedrock / Vertex AI に差し替えることも可能）

✅ キャッシュ/防御（任意→後で入れる）

- Redis（Upstashなど）
- 同じ検索の連打を防ぐ
- コストとレート制限に効く

## 4. Streamlit / Gradio / Vite の位置づけ

Streamlit / Gradio

- プロトタイプ・検証・デモに強い
- 公開もできるが「プロダクト的な作り込み」には限界が出やすい

→ 今回は「公開前提＆育てたい」ので、私の感覚通り Next.jsが自然。

Vite

- フロントSPAとしては最高
- ただし認証・SSR・ルーティング・BFFなどは自前で組むことが増えがち

→ 「B/Cで育てる」なら Next.jsの方が一体型で楽。

## 5. Botの中心設計（ここが一番大事）

私のボットは “検索そのもの” より、ここが価値：

✅ LLMを「検索オーケストレーター」にする

ユーザの自然文 → LLMが判断して → 必要なときだけ検索APIを呼ぶ

Tool（function calling）のイメージ

- search_books(params) という1ツールを用意
- LLMが状況に応じて
- title / author / subject / year / lang などを組み立てて検索
- 結果から3冊くらいを理由つき推薦

FastAPIの役割

- CiNiiのレスポンス(JSON-LD)を bot用の綺麗なJSONに正規化
- LLMに余計なデータを渡さない（トークン節約＋安定）

## 6. 開発ロードマップ（最短で完成させる順）

Phase 1（B）：まず動く公開版

1. FastAPIで /books/search を作る（appidはサーバ側環境変数）
2. Next.jsでチャットUI
3. OpenAI tool callingで search_books を呼べるようにする
4. 検索結果を整形して推薦文を返す

→ この時点で「自然会話→本を推薦」が成立する

Phase 2（C）：伸ばす（面白さ/サービス化）

- ユーザの好みを軽く記憶（セッション内でもOK）
- 本棚/お気に入り
- 推薦の精度向上（出版年やジャンル傾向の学習）
- Amazonリンク追加（ISBN検索）

Phase 3（将来像）：独自MCPサーバー化（“本検索/推薦ツール”を他クライアントでも使える形にする）

狙い（なぜMCPにするか）

- この機能を「このNext.jsのチャット」だけに閉じず、**Cursor / Claude Desktop / 自作エージェント / 別UI**から再利用できる
- Tool定義・入出力の型・エラー設計を揃えて、**LLMが扱いやすい“道具”として提供**できる
- APIキー（CiNii/OpenAI/Amazon等）をサーバ側に閉じて、クライアントはMCP越しに安全に利用できる

MCPサーバーとして提供したい“道具”（例）

- search_books：CiNiiでの書誌検索（クエリ正規化・絞り込み・ランキング）
- get_book_detail：1冊の詳細（著者/出版年/所蔵館数/目次が取れるなら目次 など）
- recommend_books：会話コンテキスト＋検索結果から推薦候補を生成（※将来的に。最初はクライアント側プロンプトでも可）
- open_in_amazon：ISBN/タイトルからAmazon検索リンク生成（制約の範囲で）

設計方針（今から揃えておくと後で楽）

- FastAPIの `/books/search` を “内部ドメインAPI” として育て、MCPはその上に薄く載せる（後からUIが増えても崩れない）
- レスポンスは **LLMに渡す前提で最小・安定**にする（長い本文やノイズは避ける）
- エラーは「ユーザ向け」ではなく **LLM/ツール呼び出し向け**にする（再試行可能/入力不正/レート制限/外部API障害を分類）
- ログとトレーシングを最初から入れる（どのtoolがどの外部APIを叩いたか）

段階的な移行プラン（壊さず進める）

- Step 0：現在のtool calling（OpenAI function calling）用の JSON schema を固定し、入出力を安定化
- Step 1：MCPの tool 定義を同じスキーマで実装（まずは `search_books` だけ）
- Step 2：Next.jsチャットは MCP 経由で同じ機能を呼ぶように切り替え（UIは変えない）
- Step 3：他クライアント（Cursor/Claude Desktop等）からも同じMCPサーバーを叩けることを確認

## 7. まとめ：私が今作るべきもの（短い一文）

私は、

「自然言語で雑談しながら、LLMがCiNii APIをfunction callingで叩いて本を推薦する公開チャットボット」

を作りたい。

そして作り方としては、

✅ Next.js（UI） + FastAPI（CiNii検索プロキシ） + OpenAI tool calling（推薦）
が最短・最適・拡張しやすい。
